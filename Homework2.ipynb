{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "75449302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "body {\n",
       "  font-family: Roboto;\n",
       "  \n",
       "}\n",
       "\n",
       "h1 {\n",
       "    font-weight: 800; \n",
       "    font-family: Roboto; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "    color: white;\n",
       "    font-family: Roboto Condensed;\n",
       "    text-shadow: 0.5pt 0.5pt 0.5pt black, 1pt 1pt 1pt black, -0.25pt -0.435pt 0.35pt hsl(0deg 0% 100% / 50%);\n",
       "    filter: drop-shadow(0.5px 0.5px 0.5px hsl(176deg 65% 10% / 75%));\n",
       "    padding: 6pt 4pt;\n",
       "    background: hsl(180deg 25% 15%);\n",
       "    background-image: radial-gradient(teal 20%, transparent 0), radial-gradient(teal 20%, transparent 0);\n",
       "    background-size: 30px 30px;\n",
       "    background-position: 0 0, 15px 15px;\n",
       "    letter-spacing: 0.25pt;\n",
       "    display: inline-flex;\n",
       "    border-radius: 6pt;\n",
       "}\n",
       "\n",
       ".myfont {\n",
       "    background: azure;\n",
       "    color: hsl(219deg 90% 58%);\n",
       "    text-shadow: 0.125pt 0.375pt 0.45pt hsl(208deg 100% 40% / 95%);\n",
       "    font-weight: 400;\n",
       "    font-family: Roboto;\n",
       "    border-radius: 4pt;\n",
       "    padding: 2pt 3pt;\n",
       "    box-shadow: 0.5pt 0.5pt 1.125pt #333;\n",
       "}\n",
       "mark {\n",
       "    background:lavender;\n",
       "    color:black;\n",
       "    font-weight: 700;\n",
       "    border-radius: 5pt;\n",
       "    padding: 2pt 3pt;\n",
       "}\n",
       "\n",
       "mark.mark2, mark.mark3 {\n",
       "    background: hsl(120deg 25% 75%);\n",
       "    font-size: 12pt;\n",
       "    box-shadow: 0.5pt 0.5pt 1pt hsl(0deg 0% 47% / 75%);\n",
       "    text-shadow: 0.125pt 0.35pt 1pt hsl(0deg 0% 0% / 92%);\n",
       "    color: white;\n",
       "    padding: 3pt 6pt;\n",
       "    font-weight: 800;\n",
       "    font-size: 11.5pt;\n",
       "    letter-spacing: 0.125pt;\n",
       "}\n",
       "\n",
       "mark.mark3 {\n",
       "    background: hsl(170deg 45% 75%);\n",
       "    font-size: 11pt;\n",
       "    padding: 2pt 4pt;\n",
       "}\n",
       "\n",
       "code.mycode {\n",
       "    background: lavender;\n",
       "    padding: 2pt 4pt;\n",
       "    border-radius: 4pt;\n",
       "    box-shadow: 0.5pt 0.5pt 1.5pt hsl(0deg 0% 34% / 75%);\n",
       "}\n",
       "\n",
       "code.mycode2 {\n",
       "    background: hsl(208deg 100% 97% / 50%);\n",
       "    font-family: Roboto Condensed;\n",
       "    color: darkslategray;\n",
       "    padding: 3pt 4pt;\n",
       "    border-radius: 4pt;\n",
       "    box-shadow: 0.5pt 0.5pt 1.5pt hsl(0deg 0% 34% / 75%);\n",
       "    font-size: 9pt;\n",
       "    line-height: 2.5;\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "from urllib.request import urlopen\n",
    "\n",
    "CSS_URL = \"https://raw.githubusercontent.com/rsomani95/jupyter-custom-theme/master/custom.css\"\n",
    "CSS_URL = \"https://raw.githubusercontent.com/hluebbering/data-512-homework_2/main/data/custom.css\"\n",
    "CSS = urlopen(CSS_URL)\n",
    "CSS = CSS.read().decode('utf-8')\n",
    "HTML_CSS = f\"\"\"\n",
    "<style>\n",
    "{CSS}\n",
    "</style>\n",
    "\"\"\"\n",
    "HTML(HTML_CSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49757702",
   "metadata": {},
   "source": [
    " \n",
    "# Homework 2. Considering Bias in Data \n",
    "\n",
    "\n",
    "\n",
    "<mark>Project Goal:</mark> The following explores the concept of data bias using Wikipedia articles considering political figures from different countries. For this project, we combine a dataset of Wikipedia articles with a dataset of country populations. We then use `ORES`, a machine learning service, to estimate the quality of each article.\n",
    "\n",
    "We perform an analysis of how the coverage of politicians on Wikipedia and the quality of articles about politicians varies among countries. The analysis consists of a series of tables that show the following:\n",
    "\n",
    "- ✅ Country coverage of politicians on Wikipedia compared to their population</br>\n",
    "- ✅ Country proportion of high quality articles about politicians</br>\n",
    "- ✅ Ranking of regions by articles-per-person and proportion of high quality articles</br>\n",
    "\n",
    "\n",
    "Lastly, the reflection focuses on how the project analysis findings and the process to reach those findings help understand the causes and consequences of biased data in large, complex data science projects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e9d132d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "body {\n",
       "  font-family: Roboto;\n",
       "  \n",
       "}\n",
       "\n",
       "h1 {\n",
       "    font-weight: 800; \n",
       "    font-family: Roboto; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "    color: white;\n",
       "    font-family: Roboto Condensed;\n",
       "    text-shadow: 0.5pt 0.5pt 0.5pt black, 1pt 1pt 1pt black, -0.25pt -0.435pt 0.35pt hsl(0deg 0% 100% / 50%);\n",
       "    filter: drop-shadow(0.5px 0.5px 0.5px hsl(176deg 65% 10% / 75%));\n",
       "    padding: 6pt 4pt;\n",
       "    background: hsl(180deg 25% 15%);\n",
       "    background-image: radial-gradient(teal 20%, transparent 0), radial-gradient(teal 20%, transparent 0);\n",
       "    background-size: 30px 30px;\n",
       "    background-position: 0 0, 15px 15px;\n",
       "    letter-spacing: 0.25pt;\n",
       "    display: inline-flex;\n",
       "    border-radius: 6pt;\n",
       "}\n",
       "\n",
       ".myfont {\n",
       "    background: azure;\n",
       "    color: hsl(219deg 90% 58%);\n",
       "    text-shadow: 0.125pt 0.375pt 0.45pt hsl(208deg 100% 40% / 95%);\n",
       "    font-weight: 400;\n",
       "    font-family: Roboto;\n",
       "    border-radius: 4pt;\n",
       "    padding: 2pt 3pt;\n",
       "    box-shadow: 0.5pt 0.5pt 1.125pt #333;\n",
       "}\n",
       "mark {\n",
       "    background:lavender;\n",
       "    color:black;\n",
       "    font-weight: 700;\n",
       "    border-radius: 5pt;\n",
       "    padding: 2pt 3pt;\n",
       "}\n",
       "\n",
       "mark.mark2, mark.mark3 {\n",
       "    background: hsl(120deg 25% 75%);\n",
       "    font-size: 12pt;\n",
       "    box-shadow: 0.5pt 0.5pt 1pt hsl(0deg 0% 47% / 75%);\n",
       "    text-shadow: 0.125pt 0.35pt 1pt hsl(0deg 0% 0% / 92%);\n",
       "    color: white;\n",
       "    padding: 3pt 6pt;\n",
       "    font-weight: 800;\n",
       "    font-size: 11.5pt;\n",
       "    letter-spacing: 0.125pt;\n",
       "}\n",
       "\n",
       "mark.mark3 {\n",
       "    background: hsl(170deg 45% 75%);\n",
       "    font-size: 11pt;\n",
       "    padding: 2pt 4pt;\n",
       "}\n",
       "\n",
       "code.mycode {\n",
       "    background: lavender;\n",
       "    padding: 2pt 4pt;\n",
       "    border-radius: 4pt;\n",
       "    box-shadow: 0.5pt 0.5pt 1.5pt hsl(0deg 0% 34% / 75%);\n",
       "}\n",
       "\n",
       "code.mycode2 {\n",
       "    background: hsl(208deg 100% 97% / 50%);\n",
       "    font-family: Roboto Condensed;\n",
       "    color: darkslategray;\n",
       "    padding: 3pt 4pt;\n",
       "    border-radius: 4pt;\n",
       "    box-shadow: 0.5pt 0.5pt 1.5pt hsl(0deg 0% 34% / 75%);\n",
       "    font-size: 9pt;\n",
       "    line-height: 2.5;\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "def css_styling(): # Styling notebook\n",
    "    styles = open(\"./data/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbbba2f",
   "metadata": {},
   "source": [
    "----------------------------------------------------\n",
    "\n",
    "\n",
    "## Step 1: Getting the Article and Population Data\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "We get data that lists <span class = \"myfont\">Wikipedia articles of politicians</span> and data for <span class = \"myfont\">country populations</span>.\n",
    "\n",
    "</div>\n",
    "\n",
    "1. <code class = \"mycode\">politicians_by_country.SEPT.2022.csv</code>: a list of article pages about politicians from different countries crawled from the Wikipedia [Category: Politicians by nationality](https://en.wikipedia.org/wiki/Category:Politicians_by_nationality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "35e61053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shahjahan Noori</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Shahjahan_Noori</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abdul Ghafar Lakanwal</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abdul_Ghafar_Lak...</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Majah Ha Adrif</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Majah_Ha_Adrif</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Haroon al-Afghani</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Haroon_al-Afghani</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tayyab Agha</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Tayyab_Agha</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                                                url  \\\n",
       "0        Shahjahan Noori      https://en.wikipedia.org/wiki/Shahjahan_Noori   \n",
       "1  Abdul Ghafar Lakanwal  https://en.wikipedia.org/wiki/Abdul_Ghafar_Lak...   \n",
       "2         Majah Ha Adrif       https://en.wikipedia.org/wiki/Majah_Ha_Adrif   \n",
       "3      Haroon al-Afghani    https://en.wikipedia.org/wiki/Haroon_al-Afghani   \n",
       "4            Tayyab Agha          https://en.wikipedia.org/wiki/Tayyab_Agha   \n",
       "\n",
       "       country  \n",
       "0  Afghanistan  \n",
       "1  Afghanistan  \n",
       "2  Afghanistan  \n",
       "3  Afghanistan  \n",
       "4  Afghanistan  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politician_articles = pd.read_csv('data/politicians_by_country_SEPT.2022.csv')\n",
    "politician_articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b734b",
   "metadata": {},
   "source": [
    "&#10148; *NOTE.* Data crawling Wikipedia to identify page subsets might result in misleading and/or duplicate category labels. Document any data inconsistencies and how to handle them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0573176",
   "metadata": {},
   "source": [
    "\n",
    "2. <code class = \"mycode\">population_by_country_2022.csv</code>: country populations data drawn from the [world population data sheet](https://www.prb.org/international/indicator/population/table) published by the Population Reference Bureau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e5abb7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geography</th>\n",
       "      <th>Population (millions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WORLD</td>\n",
       "      <td>7963.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFRICA</td>\n",
       "      <td>1419.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NORTHERN AFRICA</td>\n",
       "      <td>251.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>44.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Egypt</td>\n",
       "      <td>103.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Geography  Population (millions)\n",
       "0            WORLD                 7963.0\n",
       "1           AFRICA                 1419.0\n",
       "2  NORTHERN AFRICA                  251.0\n",
       "3          Algeria                   44.9\n",
       "4            Egypt                  103.5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_populations = pd.read_csv('data/population_by_country_2022.csv')\n",
    "country_populations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a63113b",
   "metadata": {},
   "source": [
    "----------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "## Step 2: Getting Article Quality Predictions\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Now we get the `predicted quality scores` for each article in the Wikipedia dataset using `ORES`.\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "- ORES is a machine learning tool that provides estimates of Wikipedia article quality. \n",
    "- The article quality estimates are, from best to worst:\n",
    "    - `FA:` Featured article\n",
    "    - `GA:` Good article\n",
    "    - `B:` B-class article\n",
    "    - `C:` C-class article\n",
    "    - `Start:` Start-class article\n",
    "    - `Stub:` Stub-class article\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "1. To get a Wikipedia page quality prediction from ORES for each politician’s article page: \n",
    "    1. Read each line of `politicians_by_country.SEPT.2022.csv`\n",
    "    2. Make a <code class =\"mycode2\">page info request</code> to get the current page revision id \n",
    "    3. Make an <code class =\"mycode2\">ORES request</code> using the *page title* and *current revision id*\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e9d4dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Wikipedia article pages about politicians\n",
    "politician_names = []\n",
    "for name in politician_articles['name']:\n",
    "    politician_names.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3312e3",
   "metadata": {},
   "source": [
    "<mark class=\"mark2\">MediaWiki API: Making Page Info Request</mark>\n",
    "\n",
    "ORES requires a specific revision ID of a specific article to be able to make a label prediction. We use the [API:Info](https://www.mediawiki.org/wiki/API:Info) request to get a range of metadata on an article, including the most current revision ID of the article page. The following code illustrates how to access page info data using the `MediaWiki REST API` for the EN Wikipedia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49ab590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python modules\n",
    "import json, time, urllib.parse, requests\n",
    "\n",
    "#########\n",
    "# CONSTANTS\n",
    "\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "API_LATENCY_ASSUMED = 0.002\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': 'luebhr@uw.edu, University of Washington, MSDS DATA 512 - AUTUMN 2022',\n",
    "}\n",
    "\n",
    "#PAGEINFO_EXTENDED_PROPERTIES = \"talkid|url|watched|watchers\"\n",
    "PAGEINFO_EXTENDED_PROPERTIES = \"\"\n",
    "\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",           # to simplify this should be a single page title at a time\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": PAGEINFO_EXTENDED_PROPERTIES\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c68c5992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# PROCEDURES/FUNCTIONS\n",
    "def request_pageinfo_per_article(article_title = None, \n",
    "                                 endpoint_url = API_ENWIKIPEDIA_ENDPOINT, \n",
    "                                 request_template = PAGEINFO_PARAMS_TEMPLATE,\n",
    "                                 headers = REQUEST_HEADERS):\n",
    "    if not article_title: return None \n",
    "    request_template['titles'] = article_title\n",
    "    \n",
    "    try:\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(endpoint_url, headers=headers, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb42184c",
   "metadata": {},
   "source": [
    "\n",
    "For each Wikipedia article from our list of article pages about politicians, we make a page info request to get the current page revision id used for ORES scoring. We then save the article revisions dictionary to JSON file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b436a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of Wikipedia article titles (keys) and revision IDs used for ORES scoring\n",
    "ARTICLE_REVISIONS = {}\n",
    "\n",
    "# Maintain a log of articles not able to retrieve an ORES score.\n",
    "article_without_ORES_score = []\n",
    "\n",
    "for name in politician_names:\n",
    "    \n",
    "    # Make a page info request to get the current page revision\n",
    "    info = request_pageinfo_per_article(name)    \n",
    "    \n",
    "    for i in info['query']['pages'].items():\n",
    "        \n",
    "        # If unable to get a score for a particular article\n",
    "        if i[0] == '-1':\n",
    "            \n",
    "            article_without_ORES_score.append(name)\n",
    "            continue\n",
    "        \n",
    "        # Match article title to specific revision ID\n",
    "        else:\n",
    "            item_values = i[1]\n",
    "            ARTICLE_REVISIONS[name] = item_values['lastrevid']\n",
    "\n",
    "\n",
    "# Save article revisions dictionary to JSON file\n",
    "with open(\"data/ARTICLE_REVISIONS.json\", \"w\") as outfile:\n",
    "    json.dump(ARTICLE_REVISIONS, outfile, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef500d3",
   "metadata": {},
   "source": [
    "*Note: Some articles have no score. Below is a log of articles for which we were not able to retrieve an ORES score.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c3d967d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles with no retrievable ORES score:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Prince Ofosu Sefah',\n",
       " 'Harjit Kaur Talwandi',\n",
       " 'Abd al-Razzaq al-Hasani',\n",
       " 'Kang Sun-nam',\n",
       " 'Abiodun Abimbola Orekoya',\n",
       " 'Segun “Aeroland” Adewale',\n",
       " 'Roman Konoplev',\n",
       " 'Nhlanhla “Lux” Dlamini']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Articles with no retrievable ORES score:\")\n",
    "article_without_ORES_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8870a7",
   "metadata": {},
   "source": [
    "<mark class = \"mark2\">Scores API: Making an ORES Request</mark>\n",
    "\n",
    "\n",
    "This example illustrates how to generate quality scores for article revisions using ORES. This example shows how to request a score of a specific revision, where the score provides probabilities for all of the possible article quality levels. The API documentation can be access from the main ORES page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06529aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# CONSTANTS\n",
    "\n",
    "# The current ORES API endpoint\n",
    "API_ORES_SCORE_ENDPOINT = \"https://ores.wikimedia.org/v3\"\n",
    "# A template for mapping to the URL\n",
    "API_ORES_SCORE_PARAMS = \"/scores/{context}/{revid}/{model}\"\n",
    "\n",
    "# Use some delays so that we do not hammer the API with our requests\n",
    "API_LATENCY_ASSUMED = 0.002\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': 'luebhr@uw.edu, University of Washington, MSDS DATA 512 - AUTUMN 2022'\n",
    "}\n",
    "\n",
    "# This template lists the basic parameters for making an ORES request\n",
    "ORES_PARAMS_TEMPLATE = {\n",
    "    \"context\": \"enwiki\",        # which WMF project for the specified revid\n",
    "    \"revid\" : \"\",               # the revision to be scored - this will probably change each call\n",
    "    \"model\": \"articlequality\"   # the AI/ML scoring model to apply to the reviewion\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b14d0c",
   "metadata": {},
   "source": [
    "The API request will be made using one procedure. The idea is to make this reusable. The procedure is parameterized, but relies on the constants above for the important parameters. The underlying assumption is that this will be used to request data for a set of article revisions. Therefore, the main parameter is article_revid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acaf5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# PROCEDURES/FUNCTIONS\n",
    "\n",
    "def request_ores_score_per_article(article_revid = None, \n",
    "                                   endpoint_url = API_ORES_SCORE_ENDPOINT, \n",
    "                                   endpoint_params = API_ORES_SCORE_PARAMS, \n",
    "                                   request_template = ORES_PARAMS_TEMPLATE,\n",
    "                                   headers = REQUEST_HEADERS,\n",
    "                                   features=False):\n",
    "    # Make sure we have an article revision id\n",
    "    if not article_revid: return None\n",
    "    \n",
    "    # set the revision id into the template\n",
    "    request_template['revid'] = article_revid\n",
    "    \n",
    "    # Combine endpoint_url with parameters for request URL\n",
    "    request_url = endpoint_url+endpoint_params.format(**request_template)\n",
    "    \n",
    "    # Features used by ML model can sometimes be returned as well as scores\n",
    "    if features:\n",
    "        request_url = request_url+\"?features=true\"\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb19145f",
   "metadata": {},
   "source": [
    "For each article page, we make an ORES request using the <code class =\"mycode\">page title</code> and current <code class =\"mycode\">revision id</code>. We saved this information in the `ARTICLE_REVISIONS.json` file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8cb5966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open ARTICLE_REVISIONS file\n",
    "with open('data/ARTICLE_REVISIONS.json', 'r') as openfile:\n",
    "    \n",
    "    # Read from json file\n",
    "    ARTICLE_REVISIONS_FILE = json.load(openfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0dcff612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving and including ORES data for each article\n",
    "ARTICLE_QUALITY = {}\n",
    "\n",
    "for article_name in ARTICLE_REVISIONS_FILE.keys():\n",
    "    \n",
    "    # Revision ID of article page\n",
    "    get_revid = ARTICLE_REVISIONS_FILE[article_name]\n",
    "    \n",
    "    # Make ORES request using page title and revision id\n",
    "    score = request_ores_score_per_article(get_revid)\n",
    "    unnest_score = score['enwiki']['scores']\n",
    "    \n",
    "    for i in unnest_score.values():\n",
    "        \n",
    "        # Predicted quality score for specific page\n",
    "        get_prediction = i['articlequality']['score']\n",
    "        ARTICLE_QUALITY[article_name] = get_prediction\n",
    "\n",
    "\n",
    "# Write JSON to a file\n",
    "with open(\"data/ARTICLE_QUALITY.json\", \"w\") as outfile:\n",
    "    json.dump(ARTICLE_QUALITY, outfile, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f462db40",
   "metadata": {},
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6014f5f",
   "metadata": {},
   "source": [
    "----------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Step 3: Combining the Datasets\n",
    "\n",
    "\n",
    "After retrieving and including the ORES data for each article, we merge the wikipedia and population data together. Consolidate the data into a single CSV file called `wp_politicians_by_country.csv` with the following file schema.\n",
    "\n",
    "![](schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcd3372",
   "metadata": {},
   "source": [
    "The `population_by_country_2022.csv` contains rows providing <code class =\"mycode\">cumulative regional population counts</code>. These rows are distinguished by *ALL CAPS* values under the <code class =\"mycode\">Geography</code> column. Note, a country can only exist in one region: the file represents regions in a *hierarchical order*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e8c4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_dict = {}\n",
    "region_pop = {} # Cumulative regional population counts\n",
    "country_pop = {} # Cumulative country population counts\n",
    "\n",
    "# Iterate through each row\n",
    "for index, row in country_populations.iterrows():\n",
    "    \n",
    "    if row['Geography'].isupper():\n",
    "        \n",
    "        GET_REGION = row['Geography']\n",
    "        GET_REGION_POP = row['Population (millions)']\n",
    "        REGION_COUNTRIES = []\n",
    "        \n",
    "        region_dict[GET_REGION] = REGION_COUNTRIES\n",
    "        region_pop[GET_REGION] = GET_REGION_POP\n",
    "    \n",
    "    else:\n",
    "        GET_COUNTRY = row['Geography']\n",
    "        REGION_COUNTRIES.append(GET_COUNTRY)\n",
    "        \n",
    "        country_pop[GET_COUNTRY] = float(row['Population (millions)'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5a3c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_by_region = [] # Match country to region\n",
    "\n",
    "for key, vals in region_dict.items():\n",
    "    for i in vals:\n",
    "        country_dict = {}\n",
    "        \n",
    "        country_dict['country'] = i\n",
    "        country_dict['region'] = key\n",
    "        \n",
    "        for j in country_pop:\n",
    "            if j == i:\n",
    "                country_dict['population'] = country_pop[j]\n",
    "                \n",
    "        country_by_region.append(country_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb6e6a3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Below we merge the predicted quality scores for each politician article page and the respective politician's country. We saved this information in the `ARTICLE_QUALITY.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "752730cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open ARTICLE_QUALITY file\n",
    "with open('data/ARTICLE_QUALITY.json', 'r') as openfile:\n",
    "    ARTICLE_QUALITY_FILE = json.load(openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1854b275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas dataframe to dictionary\n",
    "politician_articles_dict = politician_articles.to_dict(orient = 'records')\n",
    "page_data = [] # Merge wikipedia and population data\n",
    "\n",
    "for i in ARTICLE_QUALITY_FILE.items():\n",
    "    info_dict = {} # Page information\n",
    "    get_title = i[0] # Page title\n",
    "    get_quality = i[1] # Retrieved ORES score\n",
    "    \n",
    "    # Add predicted quality score for each article\n",
    "    info_dict['article_title'] = get_title\n",
    "    info_dict['article_quality'] = get_quality['prediction']\n",
    "    \n",
    "    # Get revision ID for article\n",
    "    for j in ARTICLE_REVISIONS_FILE.items():\n",
    "        if j[0] == get_title:\n",
    "            info_dict['revision_id'] = j[1]\n",
    "            \n",
    "    # Get country name for particular politician\n",
    "    for x in politician_articles_dict:\n",
    "        if x['name'] == get_title:\n",
    "            info_dict['country'] = x['country']        \n",
    "      \n",
    "    # Store all information for each page       \n",
    "    page_data.append(info_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d1a009",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d726bc6",
   "metadata": {},
   "source": [
    "After merging the data, we invariably run into entries which cannot be merged. So we check if population dataset has an entry for equivalent Wikipedia country, or vice-versa as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f5b0557",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries = [] # Countries in population dataset\n",
    "\n",
    "for i in country_by_region:\n",
    "    all_countries.append(i['country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e03d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_match = [] # Wikipedia countries with no matches\n",
    "new_page_data = [] # Updated page data\n",
    "\n",
    "for i in range(len(page_data)):\n",
    "    \n",
    "    get_dict = page_data[i]\n",
    "    \n",
    "    \n",
    "    # Countries with matched entry\n",
    "    if get_dict['country'] in all_countries:\n",
    "        \n",
    "        # Get population for particular country\n",
    "        for j in country_by_region:\n",
    "            if j['country'] == get_dict['country']:\n",
    "                get_dict['population'] = j['population']\n",
    "                get_dict['region'] = j['region']\n",
    "                \n",
    "        \n",
    "        new_page_data.append(get_dict)\n",
    "        \n",
    "    # Entries that cannot be merged\n",
    "    else:\n",
    "        no_match.append(get_dict['country'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ffcf6337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7456"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_page_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9c8153",
   "metadata": {},
   "source": [
    "Identify all countries for which there are no matches and output a list of those countries, with each country on a separate line called: `wp_countries-no_match.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "90c3b7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output list to text file\n",
    "with open('wp_countries-no_match.txt', 'w') as f:\n",
    "    for line in no_match:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae67064",
   "metadata": {},
   "source": [
    "Consolidate the rest of the data into a single CSV file called `wp_politicians_by_country.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "72fcdf2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_title</th>\n",
       "      <th>article_quality</th>\n",
       "      <th>revision_id</th>\n",
       "      <th>country</th>\n",
       "      <th>population</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shahjahan Noori</td>\n",
       "      <td>GA</td>\n",
       "      <td>1099689043</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>41.1</td>\n",
       "      <td>SOUTH ASIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abdul Ghafar Lakanwal</td>\n",
       "      <td>Start</td>\n",
       "      <td>943562276</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>41.1</td>\n",
       "      <td>SOUTH ASIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Majah Ha Adrif</td>\n",
       "      <td>Start</td>\n",
       "      <td>852404094</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>41.1</td>\n",
       "      <td>SOUTH ASIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Haroon al-Afghani</td>\n",
       "      <td>B</td>\n",
       "      <td>1095102390</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>41.1</td>\n",
       "      <td>SOUTH ASIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tayyab Agha</td>\n",
       "      <td>Start</td>\n",
       "      <td>1104998382</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>41.1</td>\n",
       "      <td>SOUTH ASIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7451</th>\n",
       "      <td>Rekayi Tangwena</td>\n",
       "      <td>Stub</td>\n",
       "      <td>1073818982</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>16.3</td>\n",
       "      <td>EASTERN AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7452</th>\n",
       "      <td>Josiah Tongogara</td>\n",
       "      <td>C</td>\n",
       "      <td>1106932400</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>16.3</td>\n",
       "      <td>EASTERN AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7453</th>\n",
       "      <td>Langton Towungana</td>\n",
       "      <td>Stub</td>\n",
       "      <td>904246837</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>16.3</td>\n",
       "      <td>EASTERN AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7454</th>\n",
       "      <td>Herbert Ushewokunze</td>\n",
       "      <td>Stub</td>\n",
       "      <td>959111842</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>16.3</td>\n",
       "      <td>EASTERN AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7455</th>\n",
       "      <td>Denis Walker</td>\n",
       "      <td>C</td>\n",
       "      <td>1111257734</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>16.3</td>\n",
       "      <td>EASTERN AFRICA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7456 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              article_title article_quality  revision_id      country  \\\n",
       "0           Shahjahan Noori              GA   1099689043  Afghanistan   \n",
       "1     Abdul Ghafar Lakanwal           Start    943562276  Afghanistan   \n",
       "2            Majah Ha Adrif           Start    852404094  Afghanistan   \n",
       "3         Haroon al-Afghani               B   1095102390  Afghanistan   \n",
       "4               Tayyab Agha           Start   1104998382  Afghanistan   \n",
       "...                     ...             ...          ...          ...   \n",
       "7451        Rekayi Tangwena            Stub   1073818982     Zimbabwe   \n",
       "7452       Josiah Tongogara               C   1106932400     Zimbabwe   \n",
       "7453      Langton Towungana            Stub    904246837     Zimbabwe   \n",
       "7454    Herbert Ushewokunze            Stub    959111842     Zimbabwe   \n",
       "7455           Denis Walker               C   1111257734     Zimbabwe   \n",
       "\n",
       "      population          region  \n",
       "0           41.1      SOUTH ASIA  \n",
       "1           41.1      SOUTH ASIA  \n",
       "2           41.1      SOUTH ASIA  \n",
       "3           41.1      SOUTH ASIA  \n",
       "4           41.1      SOUTH ASIA  \n",
       "...          ...             ...  \n",
       "7451        16.3  EASTERN AFRICA  \n",
       "7452        16.3  EASTERN AFRICA  \n",
       "7453        16.3  EASTERN AFRICA  \n",
       "7454        16.3  EASTERN AFRICA  \n",
       "7455        16.3  EASTERN AFRICA  \n",
       "\n",
       "[7456 rows x 6 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert list of dictionaries\n",
    "df = pd.DataFrame.from_dict(new_page_data)\n",
    "df.to_csv('wp_politicians_by_country.csv', index = False)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a30d19",
   "metadata": {},
   "source": [
    "----------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "## Step 4: Analysis\n",
    "\n",
    "The analysis consists of calculating the following on a country-by-country and regional basis:\n",
    "\n",
    "- <code class = \"mycode2\">total-articles-per-population</code> - a ratio representing the number of articles per person\n",
    "- <code class = \"mycode2\">high-quality-articles-per-population</code> - a ratio representing the number of high quality articles per person\n",
    "\n",
    "All of these values are to be \"per capita\".\n",
    "\n",
    "-  For your analysis always put a country in the closest (lowest in the hierarchy) region.\n",
    "- Keep in mind that `population_by_country_2022.csv` provides population in millions. The calculated proportions in this step are likely to be very small numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22698c6c",
   "metadata": {},
   "source": [
    "#### `total-articles-per-population`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358e387e",
   "metadata": {},
   "source": [
    "Below, we get the **total article page count** on the basis for each country (<code class = \"mycode\">country-by-country</code>) and on the basis for each region (<code class = \"mycode\">region-by-region</code>), both stored as a dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f47ea0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_per_country = {} # country-by-country basis\n",
    "articles_per_region = {} # region-by-region basis\n",
    "\n",
    "# iterate through each row\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    get_country = row['country']\n",
    "    get_region = row['region']\n",
    "    \n",
    "    \n",
    "    # Page count by country\n",
    "    if get_country in articles_per_country.keys():\n",
    "        articles_per_country[get_country] += 1\n",
    "    else:\n",
    "        articles_per_country[get_country] = 1\n",
    "     \n",
    "    \n",
    "    # Page count by region\n",
    "    if get_region in articles_per_region.keys():\n",
    "        articles_per_region[get_region] += 1\n",
    "    else:\n",
    "        articles_per_region[get_region] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f233b7",
   "metadata": {},
   "source": [
    "To calculate the ratio representing the number of articles per person on a `country-by-country` basis and a `region-by-region` basis, we take the page count and divide it by the total population for each country and each region, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a3412a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_per_country_ratio = {} # country-by-country basis\n",
    "\n",
    "for i, j in articles_per_country.items():\n",
    "    \n",
    "    country = i\n",
    "    page_count = j\n",
    "    \n",
    "    for x, y in country_pop.items():\n",
    "        if x == country:\n",
    "            get_population = y * 1000000\n",
    "            \n",
    "            if get_population == 0:\n",
    "                get_ratio = 0\n",
    "                articles_per_country_ratio[country] = get_ratio\n",
    "                \n",
    "            else:\n",
    "                get_ratio = page_count / get_population\n",
    "                articles_per_country_ratio[country] = get_ratio\n",
    "                               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ed076c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_per_region_ratio = {} # region-by-region basis\n",
    "\n",
    "for i, j in articles_per_region.items():\n",
    "    \n",
    "    region = i\n",
    "    page_count = j\n",
    "    \n",
    "    for x, y in region_pop.items():\n",
    "        if x == region:\n",
    "            region_population = y * 1000000\n",
    "            \n",
    "            if region_population == 0:\n",
    "                region_ratio = 0\n",
    "                articles_per_region_ratio[region] = region_ratio\n",
    "                \n",
    "            else:\n",
    "                region_ratio = page_count / region_population\n",
    "                articles_per_region_ratio[region] = region_ratio\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db84962",
   "metadata": {},
   "source": [
    "#### `high-quality-articles-per-population`\n",
    "\n",
    "Consider \"high quality\" articles as those that ORES predicted in either the \"FA\" (featured article) or \"GA\" (good article) classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "15fb95b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# country-by-country basis\n",
    "\n",
    "# regional basis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77553bbb",
   "metadata": {},
   "source": [
    "----------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "## Step 5: Results\n",
    "\n",
    "We produce the results from this analysis in the form of six total data tables that show:\n",
    "\n",
    "1. `Top 10 countries by coverage:` The 10 countries with the highest total articles per capita (in descending order) .\n",
    "2. `Bottom 10 countries by coverage:` The 10 countries with the lowest total articles per capita (in ascending order) .\n",
    "3. Top 10 countries by high quality: The 10 countries with the highest high quality articles per capita (in descending order).\n",
    "4. Bottom 10 countries by high quality: The 10 countries with the lowest high quality articles per capita (in ascending order).\n",
    "5. `Geographic regions by total coverage:` A rank ordered list of geographic regions (in descending order) by total articles per capita.\n",
    "6. Geographic regions by high quality coverage: Rank ordered list of geographic regions (in descending order) by high quality articles per capita.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548e0853",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. <mark class = \"mark3\">Top 10 countries by coverage:</mark>\n",
    "\n",
    "Below, we get the 10 countries with the <code class=\"mycode2\">highest total articles</code> per capita (in descending order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8d1c736d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Antigua and Barbuda', 0.00017),\n",
       " ('Federated States of Micronesia', 0.00013),\n",
       " ('Andorra', 0.0001),\n",
       " ('Barbados', 9.333333333333333e-05),\n",
       " ('Marshall Islands', 9e-05),\n",
       " ('Seychelles', 6e-05),\n",
       " ('Montenegro', 5.5e-05),\n",
       " ('Luxembourg', 5.2857142857142855e-05),\n",
       " ('Bhutan', 5.125e-05),\n",
       " ('Grenada', 5e-05)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by value\n",
    "sort_country_ratio = sorted(\n",
    "    articles_per_country_ratio.items(),\n",
    "    key=lambda item: item[1], reverse = True)\n",
    "\n",
    "sort_country_ratio[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db3053",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "2. <mark class = \"mark3\">Bottom 10 countries by coverage:</mark>\n",
    "\n",
    "Next, we get the 10 countries with the <code class=\"mycode2\">lowest total articles</code> per capita (in ascending order).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e34aa870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Romania', 1.0526315789473685e-07),\n",
       " ('Saudi Arabia', 8.174386920980926e-08),\n",
       " ('Mexico', 7.843137254901961e-09),\n",
       " ('China', 1.3921759710427398e-09),\n",
       " ('Liechtenstein', 0),\n",
       " ('Monaco', 0),\n",
       " ('Nauru', 0),\n",
       " ('Palau', 0),\n",
       " ('San Marino', 0),\n",
       " ('Tuvalu', 0)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(sort_country_ratio) \n",
    "sort_country_ratio[n-10:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0f0606",
   "metadata": {},
   "source": [
    "3. <mark class = \"mark3\">Top 10 countries by high quality:</mark> \n",
    "\n",
    "The 10 countries with the highest high quality articles per capita (in descending order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a381e707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bdc901a",
   "metadata": {},
   "source": [
    "4. <mark class = \"mark3\">Bottom 10 countries by high quality:</mark> The 10 countries with the lowest high quality articles per capita (in ascending order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517e8118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cf404e5",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "<br>\n",
    "\n",
    "5. <mark class = \"mark3\">Geographic regions by total coverage:</mark>\n",
    "\n",
    "Now, we get the ranked <code class=\"mycode2\">ordered list of geographic regions</code> (in descending order) by total articles per capita.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "adc8433f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SOUTHERN EUROPE', 5.821192052980132e-06),\n",
       " ('CARIBBEAN', 4.5454545454545455e-06),\n",
       " ('WESTERN EUROPE', 3.49746192893401e-06),\n",
       " ('EASTERN EUROPE', 2.5470383275261325e-06),\n",
       " ('NORTHERN EUROPE', 2.439252336448598e-06),\n",
       " ('WESTERN ASIA', 2.326530612244898e-06),\n",
       " ('OCEANIA', 1.9545454545454545e-06),\n",
       " ('SOUTHERN AFRICA', 1.6956521739130435e-06),\n",
       " ('EASTERN AFRICA', 1.3657505285412261e-06),\n",
       " ('SOUTH AMERICA', 1.327188940092166e-06),\n",
       " ('WESTERN AFRICA', 1.3186046511627906e-06),\n",
       " ('CENTRAL ASIA', 1.3076923076923077e-06),\n",
       " ('CENTRAL AMERICA', 1.0786516853932585e-06),\n",
       " ('MIDDLE AFRICA', 1.0357142857142857e-06),\n",
       " ('NORTHERN AFRICA', 9.043824701195219e-07),\n",
       " ('SOUTHEAST ASIA', 6.050295857988165e-07),\n",
       " ('SOUTH ASIA', 3.207171314741036e-07),\n",
       " ('EAST ASIA', 1.4516129032258064e-07)]"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by value\n",
    "sort_region_ratio = sorted(\n",
    "    articles_per_region_ratio.items(),\n",
    "    key=lambda item: item[1], reverse = True)\n",
    "\n",
    "sort_region_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f3ce52",
   "metadata": {},
   "source": [
    "6. <mark class = \"mark3\">Geographic regions by high quality coverage:</mark> \n",
    "\n",
    "Rank ordered list of geographic regions (in descending order) by high quality articles per capita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe5ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
